{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, TreeReader, TreeReaderMode, spiders_reader, cups_reader, stairs_reader\n",
    "from lambeq import TensorAnsatz, SpiderAnsatz, MPSAnsatz, AtomicType, IQPAnsatz\n",
    "from lambeq import SpacyTokeniser\n",
    "from discopy import Dim, grammar\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faults_in_file(file: str):\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser(verbose = \"progress\")\n",
    "    \n",
    "    labels, sentences = extract_data(file)\n",
    "    tokens = tokeniser.tokenise_sentences(sentences)\n",
    "\n",
    "    faults = []\n",
    "    i = 0\n",
    "    count = 0\n",
    "    while i < len(tokens):\n",
    "        try:\n",
    "            #print(f\"parsing string {i} of {len(tokens)}\")\n",
    "            diagram = parser.sentence2diagram(tokens[i], tokenised = True)\n",
    "            i += 1\n",
    "        except Exception: \n",
    "            faults.append(sentences[i])\n",
    "            print(f\"fault on sentence {i}\")\n",
    "            count += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "    return faults\n",
    "    \n",
    "def check_fixed_faults(tokens_list):\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser(verbose = \"progress\")\n",
    "    tokens = tokeniser.tokenise_sentences(tokens_list)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            print(f\"parsing sentence {i} of {len(tokens)}\")\n",
    "            diagram = parser.sentence2diagram(tokens[i], tokenised=True)\n",
    "        except Exception:\n",
    "            print(f\"Error on sentence {i}\")\n",
    "            continue\n",
    "\n",
    "    print(\"Loop done\")\n",
    "\n",
    "def find_duplicates(filename):\n",
    "    duplicates = []\n",
    "    position = 1\n",
    "    with open(filename) as f:\n",
    "        seen = set()\n",
    "        for line in f:\n",
    "            if line in seen:\n",
    "                duplicates.append( (line, position) )\n",
    "                print(line, position)\n",
    "                position += 1\n",
    "            else:\n",
    "                seen.add(line)\n",
    "                position += 1\n",
    "    \n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample >> Id(n @ n.l) @ text >> Id(n) @ Cup(n.l, n)\n"
     ]
    }
   ],
   "source": [
    "tokeniser = SpacyTokeniser()\n",
    "parser = BobcatParser(verbose = \"progress\")\n",
    "token = tokeniser.tokenise_sentence(\"sample text\")\n",
    "diagram = parser.sentence2diagram(token, tokenised = True)\n",
    "print(diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup = find_duplicates(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\GPS_edited.csv\")\n",
    "len(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fault on sentence 34\n",
      "fault on sentence 110\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#windows 11\n",
    "print(len(find_faults_in_file(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\CPN_edited.csv\")))\n",
    "print(len(find_faults_in_file(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\ePurse_edited.csv\")))\n",
    "print(len(find_faults_in_file(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\GPS_edited.csv\")))\n",
    "\n",
    "#arch linux\n",
    "print(len(find_faults_in_file(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/GPS_edited.csv\")))\n",
    "print(len(find_faults_in_file(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/CPN_edited.csv\")))\n",
    "print(len(find_faults_in_file(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/ePurse_edited.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    In questa sezione Ã¨ presente il testing per la conversione da diagrammi a circuiti.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diagrams(dataset: str):\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser(verbose = \"progress\")\n",
    "    labels, sentences = extract_data(dataset)\n",
    "    \n",
    "    tokens = tokeniser.tokenise_sentences(sentences)\n",
    "    diagrams = parser.sentences2diagrams(tokens, tokenised = True)\n",
    "    \n",
    "    return diagrams\n",
    "\n",
    "def create_circuits(diagrams: list):\n",
    "    ansatz = MPSAnsatz({AtomicType.NOUN: Dim(2), AtomicType.SENTENCE: Dim(2)}, 3)\n",
    "    circuits = [ansatz(diagram) for diagram in diagrams]\n",
    "    return circuits\n",
    "    \n",
    "def get_faults_from_diagrams(diagrams: list):\n",
    "    ansatz = TensorAnsatz({AtomicType.NOUN: Dim(2), AtomicType.SENTENCE: Dim(2), AtomicType.CONJUNCTION: Dim(2), AtomicType.PUNCTUATION: Dim(2), AtomicType.NOUN_PHRASE: Dim(2), AtomicType.PREPOSITIONAL_PHRASE: Dim(2)})\n",
    "    faults = []\n",
    "    \n",
    "    for i in range(len(diagrams)):\n",
    "        try:\n",
    "            #print(f\"circuiting diagram {i} of {len(diagrams)}\")\n",
    "            circuit = ansatz(diagrams[i])\n",
    "        except Exception:\n",
    "            faults.append(diagrams[i])\n",
    "            #print(f\"fault on diagram {i}\")\n",
    "            continue\n",
    "    \n",
    "    return faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#windows 11\n",
    "cpn_diagrams = create_diagrams(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\CPN_edited.csv\")\n",
    "epurse_diagrams = create_diagrams(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\ePurse_edited.csv\")\n",
    "gps_diagrams = create_diagrams(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\GPS_edited.csv\")\n",
    "\n",
    "\n",
    "\"\"\"#arch linux\n",
    "cpn_diagrams = create_diagrams(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/CPN_edited.csv\")\n",
    "epurse_diagrams = create_diagrams(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/ePurse_edited.csv\")\n",
    "gps_diagrams = create_diagrams(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/GPS_edited.csv\")\"\"\"\n",
    "\n",
    "\n",
    "print(f\"Diagrams parsed in cpn_diagrams: {len(cpn_diagrams)}\")\n",
    "print(f\"Diagrams parsed in epurse_diagrams: {len(epurse_diagrams)}\")\n",
    "print(f\"Diagrams parsed in gps_diagrams: {len(gps_diagrams)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "cpn_faults = get_faults_from_diagrams(cpn_diagrams)\n",
    "print(len(cpn_faults))\n",
    "\n",
    "epurse_faults = get_faults_from_diagrams(epurse_diagrams)\n",
    "print(len(epurse_faults))\n",
    "\n",
    "gps_faults = get_faults_from_diagrams(gps_diagrams)\n",
    "print(len(gps_faults))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
