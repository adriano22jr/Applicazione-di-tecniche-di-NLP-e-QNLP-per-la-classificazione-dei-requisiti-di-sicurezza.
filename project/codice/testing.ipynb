{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, TreeReader, TreeReaderMode, spiders_reader, cups_reader, stairs_reader\n",
    "from lambeq import TensorAnsatz, SpiderAnsatz, MPSAnsatz, AtomicType\n",
    "from lambeq import SpacyTokeniser\n",
    "from discopy import Dim, grammar\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_faults_in_file(file: str):\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser(verbose = \"progress\")\n",
    "    \n",
    "    labels, sentences = extract_data(file)\n",
    "    tokens = tokeniser.tokenise_sentences(sentences)\n",
    "\n",
    "    faults = []\n",
    "    i = 0\n",
    "    count = 0\n",
    "    while i < len(tokens):\n",
    "        try:\n",
    "            #print(f\"parsing string {i} of {len(tokens)}\")\n",
    "            diagram = parser.sentence2diagram(tokens[i], tokenised = True)\n",
    "            i += 1\n",
    "        except Exception: \n",
    "            faults.append(sentences[i])\n",
    "            print(f\"fault on sentence {i}\")\n",
    "            count += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "    return faults\n",
    "    \n",
    "def check_fixed_faults(tokens_list):\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser(verbose = \"progress\")\n",
    "    tokens = tokeniser.tokenise_sentences(tokens_list)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            print(f\"parsing sentence {i} of {len(tokens)}\")\n",
    "            diagram = parser.sentence2diagram(tokens[i], tokenised=True)\n",
    "        except Exception:\n",
    "            print(f\"Error on sentence {i}\")\n",
    "            continue\n",
    "\n",
    "    print(\"Loop done\")\n",
    "\n",
    "def find_duplicates(filename):\n",
    "    duplicates = []\n",
    "    position = 1\n",
    "    with open(filename) as f:\n",
    "        seen = set()\n",
    "        for line in f:\n",
    "            if line in seen:\n",
    "                duplicates.append( (line, position) )\n",
    "                print(line, position)\n",
    "                position += 1\n",
    "            else:\n",
    "                seen.add(line)\n",
    "                position += 1\n",
    "    \n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample >> Id(n @ n.l) @ text >> Id(n) @ Cup(n.l, n)\n"
     ]
    }
   ],
   "source": [
    "tokeniser = SpacyTokeniser()\n",
    "parser = BobcatParser(verbose = \"progress\")\n",
    "token = tokeniser.tokenise_sentence(\"sample text\")\n",
    "diagram = parser.sentence2diagram(token, tokenised = True)\n",
    "print(diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup = find_duplicates(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\GPS_edited.csv\")\n",
    "len(dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fault on sentence 34\n",
      "fault on sentence 110\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#windows 11\n",
    "print(len(find_faults_in_file(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\CPN_edited.csv\")))\n",
    "print(len(find_faults_in_file(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\ePurse_edited.csv\")))\n",
    "print(len(find_faults_in_file(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\GPS_edited.csv\")))\n",
    "\n",
    "#arch linux\n",
    "print(len(find_faults_in_file(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/GPS_edited.csv\")))\n",
    "print(len(find_faults_in_file(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/CPN_edited.csv\")))\n",
    "print(len(find_faults_in_file(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/ePurse_edited.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    In questa sezione Ã¨ presente il testing per la conversione da diagrammi a circuiti.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diagrams(dataset: str):\n",
    "    tokeniser = SpacyTokeniser()\n",
    "    parser = BobcatParser(verbose = \"progress\")\n",
    "    labels, sentences = extract_data(dataset)\n",
    "    \n",
    "    tokens = tokeniser.tokenise_sentences(sentences)\n",
    "    diagrams = parser.sentences2diagrams(tokens, tokenised = True)\n",
    "    \n",
    "    return diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a40156e67ad43e8853deb07a69ace85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ac591fa00c4bfd91e3c8d095eb88b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582c1f8bbc0f44a3aa47797ddd0fd15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4790bee8ac482ea63ddd73e9cb21c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803da1e26dbf43d2a8843f6f8d57e74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c822e9375bfb4b01925b0e82d0970331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62fed71c4fc4da2a38673c14fdeda2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tagging sentences:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b8bd8c781748159fb82b1eed69e196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing tagged sentences:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f98c020d494f78a0b5d9403b257fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse trees to diagrams:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagrams parsed in cpn_diagrams: 146\n",
      "Diagrams parsed in epurse_diagrams: 124\n",
      "Diagrams parsed in gps_diagrams: 168\n"
     ]
    }
   ],
   "source": [
    "#windows 11\n",
    "cpn_diagrams = create_diagrams(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\CPN_edited.csv\")\n",
    "epurse_diagrams = create_diagrams(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\ePurse_edited.csv\")\n",
    "gps_diagrams = create_diagrams(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\GPS_edited.csv\")\n",
    "\n",
    "\n",
    "\"\"\"#arch linux\n",
    "cpn_diagrams = create_diagrams(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/CPN_edited.csv\")\n",
    "epurse_diagrams = create_diagrams(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/ePurse_edited.csv\")\n",
    "gps_diagrams = create_diagrams(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/GPS_edited.csv\")\"\"\"\n",
    "\n",
    "\n",
    "print(f\"Diagrams parsed in cpn_diagrams: {len(cpn_diagrams)}\")\n",
    "print(f\"Diagrams parsed in epurse_diagrams: {len(epurse_diagrams)}\")\n",
    "print(f\"Diagrams parsed in gps_diagrams: {len(gps_diagrams)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
