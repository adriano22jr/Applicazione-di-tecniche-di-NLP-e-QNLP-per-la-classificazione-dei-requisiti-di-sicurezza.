{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, TreeReader, TreeReaderMode, spiders_reader, cups_reader, stairs_reader\n",
    "from lambeq import TensorAnsatz, SpiderAnsatz, MPSAnsatz, AtomicType\n",
    "from discopy import Dim\n",
    "from classic_pipeline import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define atomic-types\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "C = AtomicType.CONJUNCTION\n",
    "P = AtomicType.PUNCTUATION\n",
    "NP = AtomicType.NOUN_PHRASE\n",
    "PP = AtomicType.PREPOSITIONAL_PHRASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser declaration\n",
    "\n",
    "bobcat_parser = BobcatParser(verbose = \"progress\")\n",
    "spider_parser = spiders_reader\n",
    "cups_parser = cups_reader\n",
    "stairs_parser = stairs_reader\n",
    "tree_parser = TreeReader(mode=TreeReaderMode.RULE_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ansatze declaration\n",
    "\n",
    "tensor_ansatz = TensorAnsatz({N: Dim(2), S: Dim(2), C: Dim(2), P: Dim(2), NP: Dim(2), PP: Dim(2)})\n",
    "spider_ansatz = SpiderAnsatz({N: Dim(2), S: Dim(2), C: Dim(2), P: Dim(2), NP: Dim(2), PP: Dim(2)})\n",
    "mps_ansatz = MPSAnsatz({N: Dim(2), S: Dim(2), C: Dim(2), P: Dim(2), NP: Dim(2), PP: Dim(2)}, bond_dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data-extraction for classic pipeline (linux)\n",
    "\n",
    "pip = ClassicPipeline(cups_parser, tensor_ansatz)\n",
    "pip.add_rewriter_rules(ClassicPipeline.SUPPORTED_RULES[0], ClassicPipeline.SUPPORTED_RULES[1], ClassicPipeline.SUPPORTED_RULES[4])\n",
    "train_labels, train_circuits = pip.create_circuits_and_labels(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/GPS_edited.csv\", \"n\")\n",
    "test_labels, test_circuits = pip.create_circuits_and_labels(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/CPN_edited.csv\", \"n\")\n",
    "eval_labels, eval_circuits = pip.create_circuits_and_labels(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/ePurse_edited.csv\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data-extraction for classic pipeline (win11)\n",
    "\n",
    "pip = ClassicPipeline(cups_parser, tensor_ansatz)\n",
    "pip.add_rewriter_rules(ClassicPipeline.SUPPORTED_RULES[0], ClassicPipeline.SUPPORTED_RULES[1], ClassicPipeline.SUPPORTED_RULES[4])\n",
    "#train_labels, train_circuits = pip.create_circuits_and_labels(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\GPS_edited.csv\", \"n\")\n",
    "#test_labels, test_circuits = pip.create_circuits_and_labels(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\CPN_edited.csv\", \"n\")\n",
    "#eval_labels, eval_circuits = pip.create_circuits_and_labels(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\ePurse_edited.csv\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_data(\"train_data.txt\", train_labels, train_circuits)\n",
    "#save_data(\"test_data.txt\", test_labels, test_circuits)\n",
    "#save_data(\"eval_data.txt\", eval_labels, eval_circuits)\n",
    "\n",
    "train_labels, train_circuits = load_data(\"train_data.txt\")\n",
    "test_labels, test_circuits = load_data(\"test_data.txt\")\n",
    "eval_labels, eval_circuits = load_data(\"eval_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-set (GPS) faulty entries: 127:128 128:129 129:130 136:137 137:138 138:139 147:148 150:151 151:152 155:156 156:157\n",
    "#test-set (CPN) faulty entries: None\n",
    "#eval-set (ePurse) faulty entries: 22:23 34:46\n",
    "\n",
    "\n",
    "rtrain_circuits = train_circuits[0:127] + train_circuits[130:136] + train_circuits[139:147] + train_circuits[148:150] + train_circuits[152:155] + train_circuits[157:]\n",
    "reval_circuits = eval_circuits[0:22] + eval_circuits[23:34] + eval_circuits[46:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "working parser/ansatz combos:\n",
    "    - cups / (tensor, spider, mps) : works on all requirements    \n",
    "    - stairs / (tensor) : works on all requirements\n",
    "    - tree / (tensor) : works on all requirements\n",
    "    - bobcat / (tensor, spider, mps) : works with 90+% of requirements\n",
    "    \n",
    "NB: missing combos don't work together, common exception raised is AxiomError   \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training block for classical pipeline\n",
    "\n",
    "train_set, test_set, eval_set = pip.create_dataset(train_labels, train_circuits), pip.create_dataset(test_labels, test_circuits), pip.create_dataset(eval_labels, eval_circuits)\n",
    "model = pip.create_model(train_circuits, test_circuits, eval_circuits)\n",
    "#pip.create_trainer(model = model, loss = torch.nn.HingeEmbeddingLoss(), optimizer = torch.optim.Adam, n_epochs = 10)\n",
    "#pip.train_model(train_set, eval_set)\n",
    "#pip.plot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choices to test:\n",
    "    - Loss Function: for binary classification, the reccomended loss functions are BCELoss, BCEWithLogitsLoss, HingeEmbeddingLoss\n",
    "    - Epochs: range of epochs between 10 and 100\n",
    "    - Optimizer: see torch documentation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [10, 20, 40, 60 , 80, 100]\n",
    "loss_functions = [torch.nn.BCEWithLogitsLoss(), torch.nn.HingeEmbeddingLoss()]\n",
    "optimizers = [torch.optim.AdamW, torch.optim.Adadelta, torch.optim.Adagrad, torch.optim.Adam, torch.optim.Adamax] \n",
    "\n",
    "\n",
    "for epoch in epochs:\n",
    "    for loss in loss_functions:\n",
    "        for optimizer in optimizers:\n",
    "            pip.create_trainer(model = model, loss = loss, optimizer = optimizer, n_epochs = epoch)\n",
    "            pip.train_model(train_set, eval_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "class LambeqEstimator(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, epochs = 0 , loss_function = None, optimizer = None, subestimator = None):\n",
    "        self.subestimator = subestimator\n",
    "        self.subestimator.loss_function = loss_function\n",
    "        self.subestimator.epochs = epochs\n",
    "        self.subestimator.optimizer = optimizer\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self.subestimator.fit(X, y)\n",
    "        \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For multi-metric scoring, the parameter refit must be set to a scorer key or a callable to refit an estimator with the best parameter setting on the whole data and make the best_* attributes available for that metric. If this is not needed, refit should be set to False explicitly. True was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m scores \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m tuner \u001b[39m=\u001b[39m GridSearchCV(estimator \u001b[39m=\u001b[39m trainer, param_grid \u001b[39m=\u001b[39m parameters, scoring \u001b[39m=\u001b[39m scores, n_jobs \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, cv \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m tuner\u001b[39m.\u001b[39;49mfit(train_circuits, train_labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:779\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m     scorers \u001b[39m=\u001b[39m _check_multimetric_scoring(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring)\n\u001b[1;32m--> 779\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_refit_for_multimetric(scorers)\n\u001b[0;32m    780\u001b[0m     refit_metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit\n\u001b[0;32m    782\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:720\u001b[0m, in \u001b[0;36mBaseSearchCV._check_refit_for_multimetric\u001b[1;34m(self, scores)\u001b[0m\n\u001b[0;32m    713\u001b[0m valid_refit_dict \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit \u001b[39min\u001b[39;00m scores\n\u001b[0;32m    715\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    716\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    717\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m valid_refit_dict\n\u001b[0;32m    718\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit)\n\u001b[0;32m    719\u001b[0m ):\n\u001b[1;32m--> 720\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(multimetric_refit_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: For multi-metric scoring, the parameter refit must be set to a scorer key or a callable to refit an estimator with the best parameter setting on the whole data and make the best_* attributes available for that metric. If this is not needed, refit should be set to False explicitly. True was passed."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "trainer = pip.create_trainer(model = model, loss = torch.nn.BCEWithLogitsLoss, n_epochs = 10)\n",
    "#l_estimator = LambeqEstimator(trainer)\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"epochs\": [10, 20, 40, 60, 80, 100],\n",
    "    \"loss_function\": [torch.nn.BCELoss, torch.nn.BCEWithLogitsLoss, torch.nn.HingeEmbeddingLoss],\n",
    "    \"optimizer\": [torch.optim.AdamW, torch.optim.Adadelta, torch.optim.Adagrad, torch.optim.Adam, torch.optim.Adamax]    \n",
    "}\n",
    "\n",
    "scores = [\"accuracy\", \"recall\", \"f1\"]\n",
    "\n",
    "tuner = GridSearchCV(estimator = trainer, param_grid = parameters, scoring = \"f1\", n_jobs = -1, cv = 5)\n",
    "tuner.fit(train_circuits, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
