{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, TreeReader, TreeReaderMode, spiders_reader, cups_reader, stairs_reader\n",
    "from lambeq import TensorAnsatz, SpiderAnsatz, MPSAnsatz, AtomicType\n",
    "from discopy import Dim\n",
    "from classic_pipeline import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define atomic-types\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "C = AtomicType.CONJUNCTION\n",
    "P = AtomicType.PUNCTUATION\n",
    "NP = AtomicType.NOUN_PHRASE\n",
    "PP = AtomicType.PREPOSITIONAL_PHRASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser declaration\n",
    "\n",
    "bobcat_parser = BobcatParser(verbose = \"progress\")\n",
    "spider_parser = spiders_reader\n",
    "cups_parser = cups_reader\n",
    "stairs_parser = stairs_reader\n",
    "tree_parser = TreeReader(mode=TreeReaderMode.RULE_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ansatze declaration\n",
    "\n",
    "tensor_ansatz = TensorAnsatz({N: Dim(2), S: Dim(2), C: Dim(2), P: Dim(2), NP: Dim(2), PP: Dim(2)})\n",
    "spider_ansatz = SpiderAnsatz({N: Dim(2), S: Dim(2), C: Dim(2), P: Dim(2), NP: Dim(2), PP: Dim(2)})\n",
    "mps_ansatz = MPSAnsatz({N: Dim(2), S: Dim(2), C: Dim(2), P: Dim(2), NP: Dim(2), PP: Dim(2)}, bond_dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data-extraction for classic pipeline (linux)\n",
    "\n",
    "pip = ClassicPipeline(cups_parser, tensor_ansatz)\n",
    "pip.add_rewriter_rules(ClassicPipeline.SUPPORTED_RULES[0], ClassicPipeline.SUPPORTED_RULES[1], ClassicPipeline.SUPPORTED_RULES[4])\n",
    "train_labels, train_circuits = pip.create_circuits_and_labels(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/GPS_edited.csv\", \"n\")\n",
    "test_labels, test_circuits = pip.create_circuits_and_labels(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/CPN_edited.csv\", \"n\")\n",
    "eval_labels, eval_circuits = pip.create_circuits_and_labels(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/ePurse_edited.csv\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data-extraction for classic pipeline (win11)\n",
    "\n",
    "pip = ClassicPipeline(cups_parser, tensor_ansatz)\n",
    "pip.add_rewriter_rules(ClassicPipeline.SUPPORTED_RULES[0], ClassicPipeline.SUPPORTED_RULES[1], ClassicPipeline.SUPPORTED_RULES[4])\n",
    "#train_labels, train_circuits = pip.create_circuits_and_labels(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\GPS_edited.csv\", \"n\")\n",
    "#test_labels, test_circuits = pip.create_circuits_and_labels(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\CPN_edited.csv\", \"n\")\n",
    "#eval_labels, eval_circuits = pip.create_circuits_and_labels(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\ePurse_edited.csv\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_data(\"train_data.txt\", train_labels, train_circuits)\n",
    "#save_data(\"test_data.txt\", test_labels, test_circuits)\n",
    "#save_data(\"eval_data.txt\", eval_labels, eval_circuits)\n",
    "\n",
    "train_labels, train_circuits = load_data(\"train_data.txt\")\n",
    "test_labels, test_circuits = load_data(\"test_data.txt\")\n",
    "eval_labels, eval_circuits = load_data(\"eval_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-set (GPS) faulty entries: 127:128 128:129 129:130 136:137 137:138 138:139 147:148 150:151 151:152 155:156 156:157\n",
    "#test-set (CPN) faulty entries: None\n",
    "#eval-set (ePurse) faulty entries: 22:23 34:46\n",
    "\n",
    "\n",
    "rtrain_circuits = train_circuits[0:127] + train_circuits[130:136] + train_circuits[139:147] + train_circuits[148:150] + train_circuits[152:155] + train_circuits[157:]\n",
    "reval_circuits = eval_circuits[0:22] + eval_circuits[23:34] + eval_circuits[46:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "working parser/ansatz combos:\n",
    "    - cups / (tensor, spider, mps) : works on all requirements    \n",
    "    - stairs / (tensor) : works on all requirements\n",
    "    - tree / (tensor) : works on all requirements\n",
    "    - bobcat / (tensor, spider, mps) : works with 90+% of requirements\n",
    "    \n",
    "NB: missing combos don't work together, common exception raised is AxiomError   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training block for classical pipeline\n",
    "\n",
    "train_set, test_set, eval_set = pip.create_dataset(train_labels, train_circuits), pip.create_dataset(test_labels, test_circuits), pip.create_dataset(eval_labels, eval_circuits)\n",
    "model = pip.create_model(train_circuits, test_circuits, eval_circuits)\n",
    "trainer = pip.create_trainer(model = model)\n",
    "#pip.create_trainer(model = model, loss = torch.nn.HingeEmbeddingLoss(), optimizer = torch.optim.Adam, n_epochs = 10)\n",
    "#pip.train_model(train_set, eval_set)\n",
    "#pip.plot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [100]\n",
    "loss_functions = [torch.nn.BCEWithLogitsLoss(), torch.nn.HingeEmbeddingLoss()]\n",
    "optimizers = [torch.optim.AdamW, torch.optim.Adagrad, torch.optim.Adam, torch.optim.Adamax]\n",
    "learning_rates = [3e-1, 3e-2, 3e-3]\n",
    "\n",
    "pip.create_trainer(model = model, loss = loss_functions[1], optimizer = optimizers[3], n_epochs = 100, lr = learning_rates[2])\n",
    "pip.train_model(train_set, eval_set, 1, 1)\n",
    "\n",
    "\n",
    "\"\"\"#path = \"C:\\\\Users\\\\calif\\\\Desktop\\\\grid_results\\\\\"\n",
    "\n",
    "\n",
    "\n",
    "for loss in loss_functions:\n",
    "    for optimizer in optimizers:\n",
    "        for lr in learning_rates:\n",
    "            print(f\"\\nCurrent triple: \\n -LEARNING RATE: {str(lr)}\\n -LOSS FUNCTION: {str(loss)}\\n -OPTIMIZER: {str(optimizer)}\")\n",
    "            pip.create_trainer(model = model, loss = loss, optimizer = optimizer, n_epochs = 100, lr = lr)\n",
    "            pip.train_model(train_set, eval_set, 1, 1)\n",
    "            #filename = path + str(lr) + \"_\" + str(loss) + \"_\" + str(optimizer)\n",
    "            #pip.plot(filename)\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso rimuovere Adadelta optimizer dalla gridsearcg perchè non dà miglioramenti al training.\n",
    "Quando si usa HingeEmbeddedLoss ottengo loss value tendenti -infinito.\n",
    "\n",
    "\n",
    "\n",
    "Dal primo round di gridsearch:\n",
    "    - con la loss function HingeEmbeddedLoss ottengo sempre valori negativi con conseguenti score bassi nelle metriche di valutazione.\n",
    "    - con la loss function BCEWithLogitsLoss i valori sono mediamente stabili, le epoch piu equilibrate individuate sono:\n",
    "            \n",
    "            Epoch 80:   train/loss: 0.1989   valid/loss: 4.3054   train/acc: 0.8720   train/rec: 0.8571   train/f1: 0.8701   valid/acc: 0.5202   valid/rec: 0.5403   valid/f1: 0.5296     -LEARNING RATE: 0.03  -LOSS FUNCTION: BCEWithLogitsLoss() -OPTIMIZER: <'torch.optim.adamw.AdamW'>\n",
    "\n",
    "            Epoch 90:   train/loss: 0.3694   valid/loss: 6.0657   train/acc: 0.7649   train/rec: 0.7381   train/f1: 0.7584   valid/acc: 0.5242   valid/rec: 0.4919   valid/f1: 0.5083     -LEARNING RATE: 0.003 -LOSS FUNCTION: BCEWithLogitsLoss() -OPTIMIZER: <'torch.optim.adamw.AdamW'>\n",
    "\n",
    "            Epoch 95:   train/loss: 0.6820   valid/loss: 0.7440   train/acc: 0.5327   train/rec: 0.4583   train/f1: 0.4952   valid/acc: 0.5000   valid/rec: 0.4435   valid/f1: 0.4701     -LEARNING RATE: 0.003 -LOSS FUNCTION: BCEWithLogitsLoss() -OPTIMIZER: <'torch.optim.adagrad.Adagrad'>\n",
    "\n",
    "            Epoch 71:   train/loss: 0.2060   valid/loss: 1.2076   train/acc: 0.8571   train/rec: 0.8452   train/f1: 0.8554   valid/acc: 0.5363   valid/rec: 0.5000   valid/f1: 0.5188     -LEARNING RATE: 0.03 -LOSS FUNCTION: BCEWithLogitsLoss() -OPTIMIZER: <'torch.optim.adam.Adam'>\n",
    "\n",
    "\n",
    "    Le restanti combinazioni non sono riportate perchè hanno o loss troppo alte oppure score delle metriche troppo basse.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.tuner import Tuner, TuneConfig\n",
    "\n",
    "parameters = {\n",
    "    \"params\": {\n",
    "        \"epochs\": [10, 20, 40, 60, 80, 100],\n",
    "        \"loss_function\": [torch.nn.BCELoss, torch.nn.BCEWithLogitsLoss, torch.nn.HingeEmbeddingLoss],\n",
    "        \"optimizer\": [torch.optim.AdamW, torch.optim.Adadelta, torch.optim.Adagrad, torch.optim.Adam, torch.optim.Adamax]  \n",
    "    }      \n",
    "}\n",
    "\n",
    "tuner = Tuner(trainer, param_space = parameters, tune_config = TuneConfig(num_samples = 5, metric = \"train-logloss\", mode = \"min\"))\n",
    "result_grid = tuner.fit()\n",
    "best = result_grid.get_best_result()\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "trainer = pip.create_trainer(model = model, loss = torch.nn.BCEWithLogitsLoss, n_epochs = 10)\n",
    "#l_estimator = LambeqEstimator(trainer)\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"epochs\": [10, 20, 40, 60, 80, 100],\n",
    "    \"loss_function\": [torch.nn.BCELoss, torch.nn.BCEWithLogitsLoss, torch.nn.HingeEmbeddingLoss],\n",
    "    \"optimizer\": [torch.optim.AdamW, torch.optim.Adadelta, torch.optim.Adagrad, torch.optim.Adam, torch.optim.Adamax]    \n",
    "}\n",
    "\n",
    "scores = [\"accuracy\", \"recall\", \"f1\"]\n",
    "\n",
    "tuner = GridSearchCV(estimator = trainer, param_grid = parameters, scoring = \"f1\", n_jobs = -1, cv = 5)\n",
    "tuner.fit(train_circuits, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
