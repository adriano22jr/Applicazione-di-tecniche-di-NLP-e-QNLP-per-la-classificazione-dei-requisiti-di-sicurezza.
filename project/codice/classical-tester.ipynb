{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import BobcatParser, TreeReader, TreeReaderMode, spiders_reader, cups_reader, stairs_reader\n",
    "from lambeq import TensorAnsatz, SpiderAnsatz, MPSAnsatz, AtomicType\n",
    "from discopy import Dim\n",
    "from classic_pipeline import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define atomic-types\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "C = AtomicType.CONJUNCTION\n",
    "P = AtomicType.PUNCTUATION\n",
    "NP = AtomicType.NOUN_PHRASE\n",
    "PP = AtomicType.PREPOSITIONAL_PHRASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser declaration\n",
    "bobcat_parser = BobcatParser(verbose = \"progress\")\n",
    "spider_parser = spiders_reader\n",
    "cups_parser = cups_reader\n",
    "stairs_parser = stairs_reader\n",
    "tree_parser = TreeReader(mode=TreeReaderMode.RULE_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ansatze declaration\n",
    "tensor_ansatz = TensorAnsatz({N: Dim(2), S: Dim(2), C: Dim(2), P: Dim(2), NP: Dim(2), PP: Dim(2)})\n",
    "spider_ansatz = SpiderAnsatz({N: Dim(2), S: Dim(2), C: Dim(2), P: Dim(2), NP: Dim(2), PP: Dim(2)})\n",
    "mps_ansatz = MPSAnsatz({N: Dim(2), S: Dim(2), C: Dim(2), P: Dim(2), NP: Dim(2), PP: Dim(2)}, bond_dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data-extraction for classic pipeline (linux)\n",
    "\n",
    "pip = ClassicPipeline(stairs_reader, tensor_ansatz)\n",
    "train_labels, train_circuits = pip.create_circuits_and_labels(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/classical/GPS.csv\", \"n\")\n",
    "test_labels, test_circuits = pip.create_circuits_and_labels(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/classical/CPN.csv\", \"n\")\n",
    "eval_labels, eval_circuits = pip.create_circuits_and_labels(\"/home/adriano22_/Documents/GitHub/Tesi-Quantum-NLP/project/datasets/edited_datasets/classical/ePurse.csv\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data-extraction for classic pipeline (win11)\n",
    "pip = ClassicPipeline(cups_reader, tensor_ansatz)\n",
    "train_labels, train_circuits = pip.create_circuits_and_labels(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\classical\\\\GPS.csv\", \"n\")\n",
    "test_labels, test_circuits = pip.create_circuits_and_labels(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\classical\\\\CPN.csv\", \"n\")\n",
    "eval_labels, eval_circuits = pip.create_circuits_and_labels(\"C:\\\\Users\\\\calif\\\\Documents\\\\GitHub\\\\Tesi-Quantum-NLP\\\\project\\\\datasets\\\\edited_datasets\\\\classical\\\\ePurse.csv\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"train_data.txt\", train_labels, train_circuits)\n",
    "save_data(\"test_data.txt\", test_labels, test_circuits)\n",
    "save_data(\"eval_data.txt\", eval_labels, eval_circuits)\n",
    "\n",
    "#train_labels, train_circuits = load_data(\"train_data.txt\")\n",
    "#test_labels, test_circuits = load_data(\"test_data.txt\")\n",
    "#eval_labels, eval_circuits = load_data(\"eval_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-set (GPS) faulty entries: 127:128 128:129 129:130 136:137 137:138 138:139 147:148 150:151 151:152 155:156 156:157\n",
    "#test-set (CPN) faulty entries: None\n",
    "#eval-set (ePurse) faulty entries: 22:23 34:46\n",
    "\n",
    "\n",
    "rtrain_circuits = train_circuits[0:127] + train_circuits[130:136] + train_circuits[139:147] + train_circuits[148:150] + train_circuits[152:155] + train_circuits[157:]\n",
    "reval_circuits = eval_circuits[0:22] + eval_circuits[23:34] + eval_circuits[46:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "working parser/ansatz combos:\n",
    "    - cups / (tensor, spider, mps) : works on all requirements    \n",
    "    - stairs / (tensor) : works on all requirements\n",
    "    - tree / (tensor) : works on all requirements\n",
    "    - bobcat / (tensor, spider, mps) : works with 90+% of requirements\n",
    "    \n",
    "NB: missing combos don't work together, common exception raised is AxiomError   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training block for classical pipeline\n",
    "train_set, test_set, eval_set = pip.create_dataset(train_circuits, train_labels), pip.create_dataset(test_circuits, test_labels), pip.create_dataset(eval_circuits, eval_labels)\n",
    "model = pip.create_model(train_circuits, test_circuits, eval_circuits)\n",
    "pip.create_trainer(model = model, loss = torch.nn.BCEWithLogitsLoss(), optimizer = torch.optim.Adam, n_epochs = 100, lr = 3e-2, evaluate = True)\n",
    "pip.train_and_evaluate(train_set, eval_set, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [100]\n",
    "loss_functions = [torch.nn.BCEWithLogitsLoss(), torch.nn.HingeEmbeddingLoss()]\n",
    "optimizers = [torch.optim.AdamW, torch.optim.Adagrad, torch.optim.Adam, torch.optim.Adamax]\n",
    "learning_rates = [3e-1, 3e-2, 3e-3]\n",
    "\n",
    "train_set, test_set, eval_set = pip.create_dataset(train_circuits, train_labels), pip.create_dataset(test_circuits, test_labels), pip.create_dataset(eval_circuits, eval_labels)\n",
    "model = pip.create_model(train_circuits, test_circuits, eval_circuits)\n",
    "\n",
    "for loss in loss_functions:\n",
    "    for optimizer in optimizers:\n",
    "        for lr in learning_rates:\n",
    "            print(f\"\\nCurrent triple: \\n -LEARNING RATE: {str(lr)}\\n -LOSS FUNCTION: {str(loss)}\\n -OPTIMIZER: {str(optimizer)}\")\n",
    "            pip.create_trainer(model = model, loss = loss, optimizer = optimizer, n_epochs = 100, lr = lr, evaluate = True)\n",
    "            pip.train_and_evaluate(train_set, eval_set, 1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso rimuovere Adadelta optimizer dalla gridsearcg perchè non dà miglioramenti al training.\n",
    "Quando si usa HingeEmbeddedLoss ottengo loss value tendenti -infinito.\n",
    "\n",
    "\n",
    "\n",
    "Dal primo round di gridsearch:\n",
    "    - con la loss function HingeEmbeddedLoss ottengo sempre valori negativi con conseguenti score bassi nelle metriche di valutazione.\n",
    "    - con la loss function BCEWithLogitsLoss i valori sono mediamente stabili, le epoch piu equilibrate individuate sono:\n",
    "            \n",
    "            Epoch 80:   train/loss: 0.1989   valid/loss: 4.3054   train/acc: 0.8720   train/rec: 0.8571   train/f1: 0.8701   valid/acc: 0.5202   valid/rec: 0.5403   valid/f1: 0.5296     -LEARNING RATE: 0.03  -LOSS FUNCTION: BCEWithLogitsLoss() -OPTIMIZER: <'torch.optim.adamw.AdamW'>\n",
    "\n",
    "            Epoch 90:   train/loss: 0.3694   valid/loss: 6.0657   train/acc: 0.7649   train/rec: 0.7381   train/f1: 0.7584   valid/acc: 0.5242   valid/rec: 0.4919   valid/f1: 0.5083     -LEARNING RATE: 0.003 -LOSS FUNCTION: BCEWithLogitsLoss() -OPTIMIZER: <'torch.optim.adamw.AdamW'>\n",
    "\n",
    "            Epoch 95:   train/loss: 0.6820   valid/loss: 0.7440   train/acc: 0.5327   train/rec: 0.4583   train/f1: 0.4952   valid/acc: 0.5000   valid/rec: 0.4435   valid/f1: 0.4701     -LEARNING RATE: 0.003 -LOSS FUNCTION: BCEWithLogitsLoss() -OPTIMIZER: <'torch.optim.adagrad.Adagrad'>\n",
    "\n",
    "            Epoch 71:   train/loss: 0.2060   valid/loss: 1.2076   train/acc: 0.8571   train/rec: 0.8452   train/f1: 0.8554   valid/acc: 0.5363   valid/rec: 0.5000   valid/f1: 0.5188     -LEARNING RATE: 0.03 -LOSS FUNCTION: BCEWithLogitsLoss() -OPTIMIZER: <'torch.optim.adam.Adam'>\n",
    "\n",
    "\n",
    "    Le restanti combinazioni non sono riportate perchè hanno o loss troppo alte oppure score delle metriche troppo basse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import Decimal\n",
    "\n",
    "def extract(filename):\n",
    "    file = open(filename, \"r\")\n",
    "    dictionary = {\n",
    "        \"train_loss\": list(),\n",
    "        \"test_loss\": [],\n",
    "        \"train_acc\" : [],\n",
    "        \"test_acc\" : [],\n",
    "        \"train_rec\" : [],\n",
    "        \"test_rec\" : [],\n",
    "        \"train_f1\" : [],\n",
    "        \"test_f1\" : []\n",
    "    }\n",
    "    \n",
    "    for line in file:\n",
    "        lista = line.split()\n",
    "        dictionary[\"train_loss\"] += [float(lista[3])]\n",
    "        dictionary[\"test_loss\"] += [float(lista[5])]\n",
    "        dictionary[\"train_acc\"] += [float(lista[7])]\n",
    "        dictionary[\"test_acc\"] += [float(lista[13])]\n",
    "        dictionary[\"train_rec\"] += [float(lista[9])]\n",
    "        dictionary[\"test_rec\"] += [float(lista[15])]\n",
    "        dictionary[\"train_f1\"] += [float(lista[11])]\n",
    "        dictionary[\"test_f1\"] += [float(lista[17])]\n",
    "        \n",
    "    return dictionary\n",
    "    \n",
    "def calculate_precision(f1, rec):\n",
    "    value =  - ((f1 * rec) / (f1 - 2*rec))\n",
    "    rounded = round(Decimal(value), 4)\n",
    "    return rounded\n",
    "    \n",
    "def add_precision(diz):\n",
    "    train_prec = []\n",
    "    test_prec = []\n",
    "    \n",
    "    for item1, item2 in zip(diz[\"train_rec\"], diz[\"train_f1\"]):\n",
    "        train_prec.append(calculate_precision(item2, item1))\n",
    "        \n",
    "    for item1, item2 in zip(diz[\"test_rec\"], diz[\"test_f1\"]):\n",
    "        test_prec.append(calculate_precision(item2, item1))\n",
    "        \n",
    "    diz[\"train_prec\"] = train_prec\n",
    "    diz[\"test_prec\"] = test_prec\n",
    "    \n",
    "    return diz\n",
    "    \n",
    "def plot_loss(dictionary):\n",
    "    x = [i for i in range(0, 100, 1)]\n",
    "    figure, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 20))\n",
    "    \n",
    "    ax1.plot(x, dictionary[\"train_loss\"], label = \"train loss\")\n",
    "    ax2.plot(x, dictionary[\"test_loss\"], label = \"test loss\", color = \"orange\")\n",
    "    \n",
    "    ax1.legend(loc = 'upper right'), ax2.legend(loc = 'upper right')\n",
    "    ax1.set_xticks([x for x in range(0, 105, 5)]), ax2.set_xticks([x for x in range(0, 105, 5)])\n",
    "    ax1.set_yticks([y/100 for y in range(0, 100, 5)]), ax2.set_yticks([y/100 for y in range(0, 100, 5)])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_data(dictionary):\n",
    "    x = [i for i in range(0, 100, 1)]\n",
    "    figure, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 12))\n",
    "  \n",
    "    ax1.plot(x, dictionary[\"train_acc\"], label = \"train accuracy\")\n",
    "    ax1.plot(x, dictionary[\"test_acc\"], label = \"test accuracy\", color = \"orange\")\n",
    "    ax4.plot(x, dictionary[\"train_prec\"], label = \"train precision\")\n",
    "    ax4.plot(x, dictionary[\"test_prec\"], label = \"test precision\", color = \"orange\")\n",
    "    ax2.plot(x, dictionary[\"train_rec\"], label = \"train recall\")\n",
    "    ax2.plot(x, dictionary[\"test_rec\"], label = \"test recall\", color = \"orange\")\n",
    "    ax3.plot(x, dictionary[\"train_f1\"], label = \"train f1\")\n",
    "    ax3.plot(x, dictionary[\"test_f1\"], label = \"test f1\", color = \"orange\")       \n",
    "    \n",
    "    ax1.legend(loc = 'upper right'), ax2.legend(loc = 'upper right'), ax3.legend(loc = 'upper right'), ax4.legend(loc = 'upper right')\n",
    "    ax1.set_xticks([x for x in range(0, 105, 5)]), ax2.set_xticks([x for x in range(0, 105, 5)]), ax3.set_xticks([x for x in range(0, 105, 5)]), ax4.set_xticks([x for x in range(0, 105, 5)])     \n",
    "    ax1.set_yticks([y / 100 for y in range(30, 100, 5)]), ax2.set_yticks([y / 100 for y in range(30, 100, 5)]), ax3.set_yticks([y / 100 for y in range(30, 100, 5)]), ax4.set_yticks([y / 100 for y in range(30, 100, 5)])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz = extract(\"plot.txt\") \n",
    "plot_data(diz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
